<!DOCTYPE html>

<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>CSA: Bridging Modalities with Unimodal Power</title>
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Sans+Pro:300,300i,600">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,300,400italic' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Roboto" />
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
  <link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" rel="stylesheet"
    integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" crossorigin="anonymous">
  <script type="text/javascript"
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  <script src="//code.jquery.com/jquery-1.12.0.min.js"></script>
  <link href="https://utaustin-swarmlab.github.io/css/zoom.css" rel="stylesheet">
  <script src="https://utaustin-swarmlab.github.io/js/zoom.js"></script>
  <script src="https://utaustin-swarmlab.github.io/js/transition.js"></script>

  <link rel="stylesheet" href="/style.css">
  <!-- favicon -->
  <!-- <link rel="shortcut icon" href="https://utaustin-swarmlab.github.io/favicon.ico"> -->
  <link rel="shortcut icon" href="https://utaustin-swarmlab.github.io/images/logo/swarm-lab-landing-image.png">
  <!-- <link rel="apple-touch-icon-precomposed" href="https://utaustin-swarmlab.github.io/images/logo/logo_square.jpg">
  <link rel="apple-touch-icon-precomposed" sizes="72x72" href="https://utaustin-swarmlab.github.io/images/logo/logo-icon-72.png">
  <link rel="apple-touch-icon-precomposed" sizes="114x114" href="https://utaustin-swarmlab.github.io/images/logo/logo-icon-114.png">
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://utaustin-swarmlab.github.io/images/logo/logo-icon-144.png"> -->

  <!-- Add favourite fonts here -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link
    href="https://fonts.googleapis.com/css2?family=Fira+Code:wght@300;400;500;600;700&family=Source+Sans+Pro:ital,wght@0,200;0,300;0,400;0,600;0,700;0,900;1,200;1,300;1,400;1,600;1,700;1,900&family=Source+Serif+Pro:ital,wght@0,200;0,300;0,400;0,600;0,700;0,900;1,200;1,300;1,400;1,600;1,700;1,900&display=swap"
    rel="stylesheet">
  <!-- open graph -->

  
  
  <meta property="og:locale" content="en_US">
  <meta property="og:type" content="article">
  <meta property="og:url" content="https://utaustin-swarmlab.github.io/2025/01/24/CSA.html">
  <meta property="og:title" content="CSA: Bridging Modalities with Unimodal Power">
  <meta property="og:site_name" content="Swarm Lab at UT Austin">
  <meta property="og:description" content="ICLR 2025 paper"/>
  <meta property="og:image" content="https://utaustin-swarmlab.github.io/images/logo/logo_square.jpg">

</head>

<script>
  /**
  * RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
  * LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
  */
  /*
  var disqus_config = function () {
  this.page.url = PAGE_URL; // Replace PAGE_URL with your page's canonical URL variable
  this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
  };
  */
  (function () { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');

    s.src = '//kordinglab.disqus.com/embed.js';

    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments
    powered by Disqus.</a></noscript>

<body id="page-top" style="padding-top: 7em;">

  <nav class="navbar navbar-default navbar-fixed-top">
        <div class="container">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class='navbar-brand page-scroll' href='/'>
              <span style="font-family: 'Source Sans Pro', sans-serif; font-weight:600; color: #FFFFFF;">Swarm Lab</span>
              </a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <ul class="nav navbar-nav navbar-right">
                      
                      <li><a class='name' href="/" style="font-family: 'Source Sans Pro', sans-serif; color: #FFFFFF; font-weight: 500;">HOME</a></li>
                      
                      <li><a class='name' href="/about" style="font-family: 'Source Sans Pro', sans-serif; color: #FFFFFF; font-weight: 500;">ABOUT</a></li>
                      
                      <li><a class='name' href="/publications" style="font-family: 'Source Sans Pro', sans-serif; color: #FFFFFF; font-weight: 500;">PUBLICATIONS</a></li>
                      
                      <li><a class='name' href="/people" style="font-family: 'Source Sans Pro', sans-serif; color: #FFFFFF; font-weight: 500;">PEOPLE</a></li>
                      
                      <li><a class='name' href="/lab_pictures" style="font-family: 'Source Sans Pro', sans-serif; color: #FFFFFF; font-weight: 500;">LAB PICTURES</a></li>
                      
                      <li><a class='name' href="/blog" style="font-family: 'Source Sans Pro', sans-serif; color: #FFFFFF; font-weight: 500;">BLOG</a></li>
                      
                    </ul>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container-fluid -->
</nav>

  <div class="container content post">
  <h3><strong>CSA: Bridging Modalities with Unimodal Power</strong><br/>
  <small>posted on January 24, 2025</small></h3>
  <hr>
  <p><em>By Po-han</em></p>

<h1 id="csa-bridging-modalities-with-unimodal-power">CSA: Bridging Modalities with Unimodal Power</h1>

<p>Proceedings of the International Conference on Learning Representations (ICLR)</p>

<p><strong>TLDR:</strong> This post introduces <em>CSA</em>, a novel method for bridging modalities with unimodal power. CSA leverages existing unimodal encoders to create a shared multimodal feature space, achieving competitive results with minimal training data and computational resources.</p>

<p><a href="https://arxiv.org/abs/2410.07610">arXiv link</a></p>

<p><a href="https://github.com/UTAustin-SwarmLab/Multi-modal-Data-Alignment">Code</a></p>

<h2 id="introduction">Introduction</h2>
<p>In the world of artificial intelligence, multimodal models like CLIP have shown impressive capabilities in tasks ranging from image classification to cross-modal retrieval. However, these models often demand enormous amounts of training data, which can be a significant challenge. What if we could achieve similar results with far less data and computation? That is the question that led us to develop Canonical Similarity Analysis (CSA).</p>

<h2 id="the-challenge-of-multimodal-learning">The Challenge of Multimodal Learning</h2>
<p>Multimodal models aim to create a shared feature space where data from different modalities (like images and text) can be compared. The traditional approach involves training models on massive datasets of paired multimodal data. For example, the original CLIP model was trained on 400 million image-text pairs, requiring significant computational resources. This approach also faces challenges with data quality, as internet-sourced data can be noisy or mislabeled.</p>

<h2 id="our-approach-leveraging-unimodal-encoders">Our Approach: Leveraging Unimodal Encoders</h2>
<p>We hypothesized that we could build powerful multimodal encoders by leveraging existing, well-developed unimodal encoders. Unimodal encoders, like DINO for images and GTR for text, are trained on single modalities and require significantly less data than their multimodal counterparts. Our idea is to map the features from these unimodal encoders into a shared multimodal feature space using limited data.</p>

<h3 id="system-plot">System Plot:</h3>
<figure style="text-align: center;">
    <img src="/images/post/CSA_system_graph.png" alt="CSA System Plot" height="auto" style="margin: auto; display: block;" />
   <figcaption>CSA: System Overview</figcaption>
   <p></p>
</figure>

<p>The CSA system works by first using pre-trained unimodal encoders to extract features from different modalities, such as images and text. These encoders, which have already been trained on large datasets of single-modality data, create a representation of the input in a feature space specific to their modality. Then, Canonical Correlation Analysis (CCA) is applied to these unimodal features. CCA identifies the dimensions of each unimodal feature space that are most correlated with each other. It finds linear transformations that project the unimodal features into a shared, lower-dimensional space. Finally, CSA calculates a weighted cosine similarity between the transformed features of different modalities, using the correlation coefficients derived from CCA as weights. This similarity score is used for various downstream tasks, such as classification and retrieval. The hyperparameter s controls how many dimensions are considered when calculating the weighted cosine similarity, allowing for a trade-off between noise reduction and retention of information. The entire process is achieved without any training of neural networks, making it highly efficient.</p>

<h2 id="introducing-canonical-similarity-analysis-csa">Introducing Canonical Similarity Analysis (CSA)</h2>
<p>To achieve this, we developed Canonical Similarity Analysis (CSA). CSA uses two unimodal encoders to encode data into unimodal features, and then projects these features into a joint multimodal feature space. The core of CSA is its method for mapping unimodal features to a multimodal space and a novel similarity function for replicating CLIP’s similarity score. We use Canonical Correlation Analysis (CCA) to find the bases in each unimodal feature space that maximize correlation. This process involves a matrix decomposition without the need for training neural networks, making CSA computationally efficient. We then calculate a weighted cosine similarity in this space. By discarding information from less correlated bases, CSA focuses on the most relevant multimodal information. How CSA Works</p>
<ul>
  <li>Unimodal Encoding: First, we use pre-trained unimodal encoders to extract features from each modality.</li>
  <li>CCA Mapping: Next, we use CCA to find the optimal linear transformations that map these features into a common space. CCA identifies the dimensions of the unimodal feature spaces that are most correlated and obtain the correlation coefficients.</li>
  <li>Weighted Cosine Similarity: We then calculate a weighted cosine similarity score to measure the similarity of two multimodal data points, using the correlation coefficients as weights.</li>
</ul>

<h2 id="data-efficiency-and-performance">Data Efficiency and Performance</h2>
<p>One of the most remarkable features of CSA is its data efficiency. In our experiments, CSA matched or exceeded the performance of CLIP in image classification, mislabeled data detection, and misinformation detection while requiring $50,000$ times less paired multimodal data for fine-tuning.</p>
<ul>
  <li>On ImageNet classification, CSA only needs $35,000$ training samples to match CLIP’s performance.</li>
  <li>In Leafy Spurge classification, CSA outperformed CLIP with only $800$ training images.</li>
  <li>CSA also showed superior performance in detecting mislabeled ImageNet images compared to CLIP, ASIF, and LLaVA.</li>
  <li>We tested CSA on detecting misinformative news captions from the COSMOS dataset, and it outperformed both CLIP and ASIF.</li>
  <li>We also showed that CSA is robust to noisy data. Even with $50\%$ of training labels randomly shuffled, CSA maintained a high level of accuracy.</li>
  <li>We’ve demonstrated CSA’s versatility with modalities beyond image and text, including audio and text, paving the way for new modality pairs, such as lidar and text.</li>
</ul>

<h2 id="limitations-and-future-directions">Limitations and Future Directions</h2>
<p>While CSA is effective, it currently focuses on bimodal data, unlike models like ImageBind, which can handle six modalities. In addition, the performance of CSA relies on the quality of the unimodal encoders used.
Our future work will explore:</p>
<ul>
  <li>Extending CSA to more than two modalities.</li>
  <li>Understanding the relationship between the training set size and performance.</li>
  <li>Fine-tuning the unimodal encoders for improved feature spaces.</li>
  <li>Applying CSA to intramodal data, such as multi-view images.</li>
</ul>

<h2 id="conclusion">Conclusion</h2>
<p>CSA offers a compelling solution to the challenges of multimodal learning. By leveraging the power of unimodal encoders and using a novel similarity analysis method, CSA achieves competitive results with minimal training data and computational resources. We believe that CSA opens up new possibilities for efficient and robust multimodal machine learning towards binding emerging modalities.</p>

  <ul class="share-buttons">
  <a style="color:#3b5998;" href="https://www.facebook.com/sharer/sharer.php?u=https://utaustin-swarmlab.github.io/2025/01/24/CSA.html" title="Share on Facebook" target="_blank"><i class="fa fa-facebook-square fa-2x"></i></a>
  <a style="color:#0084b4;" href="https://twitter.com/intent/tweet?text=https://utaustin-swarmlab.github.io/2025/01/24/CSA.html" title="Share on Twitter" target="_blank"><i class="fa fa-twitter-square fa-2x"></i></a>
</ul>

  <!-- <div id="disqus_thread"></div> -->
</div>


  <script id="dsq-count-scr" src="//kordinglab.disqus.com/count.js" async></script>
  <script src="http://code.jquery.com/jquery-2.2.1.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"
    integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS"
    crossorigin="anonymous"></script>
  <script type="text/javascript">
    var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
    document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
  </script>
  <script type="text/javascript">
    var pageTracker = _gat._getTracker("UA-34145995-1");
    pageTracker._trackPageview();
  </script>

  <!-- Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-34145995-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'UA-34145995-1');
  </script>

</body>

</html>
